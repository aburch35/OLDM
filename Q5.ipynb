{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Read Intel.csv and Microsoft.csv. Note that they must be in the same directory as this Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize the \"intel_data\" dictionary\n",
    "intel_data = {\"days\": [],         # days\n",
    "              \"SVM\": [],          # support vector machine prediction\n",
    "              \"RF\": [],           # random forest prediction\n",
    "              \"KNN\": [],          # k-nearest neighbor prediction\n",
    "              \"ARIMA\": [],        # autoregressive integrated moving average prediction\n",
    "              \"MA\": [],           # moving average prediction\n",
    "              \"actual\": []}       # actual closing price\n",
    "\n",
    "# read the data from Intel.csv and store it in the dictionary called intel_data\n",
    "with open('Intel.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for idx, row in enumerate(reader):\n",
    "        if idx > 0:\n",
    "            intel_data['days'].append(int(row[0]))\n",
    "            intel_data['actual'].append(float(row[1]))\n",
    "            intel_data['RF'].append(float(row[2]))\n",
    "            intel_data['KNN'].append(float(row[3]))\n",
    "            intel_data['SVM'].append(float(row[4]))\n",
    "            intel_data['ARIMA'].append(float(row[5]))\n",
    "            intel_data['MA'].append(float(row[6]))\n",
    "\n",
    "# print the \"intel_data\" dictionary\n",
    "#print(f\"intel_data = {intel_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize the \"microsoft_data\" dictionary\n",
    "microsoft_data = {\"days\": [],         # days\n",
    "                  \"SVM\": [],          # support vector machine prediction\n",
    "                  \"RF\": [],           # random forest prediction\n",
    "                  \"KNN\": [],          # k-nearest neighbor prediction\n",
    "                  \"ARIMA\": [],        # autoregressive integrated moving average prediction\n",
    "                  \"MA\": [],           # moving average prediction\n",
    "                  \"actual\": []}       # actual closing price\n",
    "\n",
    "# read the data from Microsoft.csv and store it in the dictionary called microsoft_data\n",
    "with open('Microsoft.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for idx, row in enumerate(reader):\n",
    "        if idx > 0:\n",
    "            microsoft_data['days'].append(int(row[0]))\n",
    "            microsoft_data['actual'].append(float(row[1]))\n",
    "            microsoft_data['RF'].append(float(row[2]))\n",
    "            microsoft_data['KNN'].append(float(row[3]))\n",
    "            microsoft_data['SVM'].append(float(row[4]))\n",
    "            microsoft_data['ARIMA'].append(float(row[5]))\n",
    "            microsoft_data['MA'].append(float(row[6]))\n",
    "\n",
    "# print the \"microsoft_data\" dictionary\n",
    "#print(f\"microsoft_data = {microsoft_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(prediction, actual):\n",
    "    # prediction: prediction value\n",
    "    # actual: actual value\n",
    "    \n",
    "    return min(np.abs(prediction - actual) / actual, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Implement the multiplicative weight update algorithm (MWU) on Intel stock prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experts' weights for day  1 :  [0.9973072196701802, 0.9897350268446107, 0.9970759894253655, 0.9994691707412445, 0.9964875878952737]\n",
      "Current experts' weights for day  2 :  [0.9946227245839067, 0.9798643778024086, 0.9948383376779992, 0.9990742339944403, 0.9939650074957774]\n",
      "Current experts' weights for day  3 :  [0.9920443469294652, 0.9700976770518838, 0.9931358686064287, 0.9988491301870122, 0.9923603078938854]\n",
      "Current experts' weights for day  4 :  [0.9913345234208161, 0.9634642930063168, 0.992985283646, 0.9970846951833204, 0.9912199820502224]\n",
      "Current experts' weights for day  5 :  [0.9822430301476753, 0.9630600164243907, 0.9866130045261292, 0.985222318127281, 0.9794153317304559]\n",
      "Current experts' weights for day  6 :  [0.9788130096870966, 0.9586050173434684, 0.980214584448635, 0.9705768112338891, 0.9652846173436292]\n",
      "Current experts' weights for day  7 :  [0.9781159521582171, 0.9552619174195984, 0.9726652248715685, 0.9559201143605242, 0.9534984636500629]\n",
      "Current experts' weights for day  8 :  [0.976756745817803, 0.947263766779851, 0.9639616003702224, 0.9408025500336629, 0.9441625892618544]\n",
      "Current experts' weights for day  9 :  [0.9748552407883776, 0.9360617764801002, 0.9551473409672113, 0.925397038214166, 0.9373788102479483]\n",
      "Current experts' weights for day  10 :  [0.9742770975849923, 0.923266994834028, 0.942388053108695, 0.9126695526040363, 0.935857267469042]\n",
      "Current experts' weights for day  11 :  [0.972432608583778, 0.9155443473315866, 0.9326508249866942, 0.8989716304573382, 0.9357805257129018]\n",
      "Current experts' weights for day  12 :  [0.9708940661257622, 0.9059150152203449, 0.9229839044143161, 0.8838925059435695, 0.9347387102051868]\n",
      "Current experts' weights for day  13 :  [0.9671654427930537, 0.8988988222504289, 0.9139024487665695, 0.8663953264433416, 0.9312761568304142]\n",
      "Current experts' weights for day  14 :  [0.959515132567442, 0.8908984206652623, 0.9068091839177324, 0.8433373148359301, 0.9226225983647542]\n",
      "Current experts' weights for day  15 :  [0.9576587248899289, 0.8765101623206474, 0.8928407348460756, 0.8198493041949203, 0.9137398609727078]\n",
      "Current experts' weights for day  16 :  [0.9536144933166847, 0.8606469879908714, 0.87930725417463, 0.7935700681857143, 0.9033245277020083]\n",
      "Current experts' weights for day  17 :  [0.9529297359696076, 0.8432456937335682, 0.8616495076452717, 0.769014161665421, 0.896161120683765]\n",
      "Current experts' weights for day  18 :  [0.951586035078005, 0.8251933073838593, 0.8436225142909554, 0.7461440247545607, 0.8925466152160659]\n",
      "Current experts' weights for day  19 :  [0.9502166662935054, 0.8074859973419316, 0.8262934379209647, 0.7236971848436196, 0.8908450316128343]\n",
      "Current experts' weights for day  20 :  [0.9472096879321981, 0.7915613868498363, 0.8106419231337965, 0.700519021023804, 0.8892891783708139]\n",
      "Current experts' weights for day  21 :  [0.9443638934943239, 0.7778897396696786, 0.7959315961165552, 0.6751950363278804, 0.8854184472759898]\n",
      "Current experts' weights for day  22 :  [0.9438854608716453, 0.7631627748779278, 0.7764385573728351, 0.6525867145287543, 0.8839566910889661]\n",
      "Current experts' weights for day  23 :  [0.9421748958429518, 0.7491582416602897, 0.7599314952245513, 0.6297654445029844, 0.8820401138704809]\n",
      "Current experts' weights for day  24 :  [0.9417433816775201, 0.7345600992695555, 0.7420759135482403, 0.6086973963782752, 0.8814815338783109]\n",
      "Current experts' weights for day  25 :  [0.9412988602918956, 0.7205206047066773, 0.7244762931585523, 0.5883524785493435, 0.8813193578891463]\n",
      "Current experts' weights for day  26 :  [0.9384493414782673, 0.7081059812893478, 0.710083568103007, 0.5668318928553068, 0.8798670123614055]\n",
      "Current experts' weights for day  27 :  [0.9373539286870013, 0.6952925583864271, 0.6930593008312745, 0.5472024730973324, 0.8798550761686685]\n",
      "Current experts' weights for day  28 :  [0.9372095200622915, 0.6819778312418446, 0.6762449927904579, 0.5290274248244571, 0.8788636893805375]\n",
      "Current experts' weights for day  29 :  [0.935594493627287, 0.6701248479676487, 0.6622136701153692, 0.5103038860826966, 0.8785843073208576]\n",
      "Current experts' weights for day  30 :  [0.9327520167850976, 0.6584964577047797, 0.6483313075972126, 0.49127402271022635, 0.8768336061461812]\n",
      "Current experts' weights for day  31 :  [0.9323651635279371, 0.6451587158998896, 0.6320053759374392, 0.474912771806165, 0.8757334025803876]\n",
      "Current experts' weights for day  32 :  [0.9319659127848443, 0.6327861919508777, 0.6180347418020197, 0.45871275299007536, 0.8746640465625113]\n",
      "Current experts' weights for day  33 :  [0.9312524409379132, 0.6200051956203146, 0.6041121547183254, 0.44377491961649274, 0.8730155708456891]\n",
      "Current experts' weights for day  34 :  [0.9288201530310629, 0.6063547295464453, 0.5901442238813833, 0.43047359373344274, 0.8697609675888965]\n",
      "Current experts' weights for day  35 :  [0.9283049431893134, 0.5922046028912419, 0.5774464916176609, 0.4184547342045252, 0.8652783543906188]\n",
      "Current experts' weights for day  36 :  [0.9280124535092025, 0.5784716701901234, 0.5667665641906255, 0.4067928724171103, 0.8614663297132044]\n",
      "Current experts' weights for day  37 :  [0.926469367484239, 0.5656887019196197, 0.5571652742612397, 0.3948257409489548, 0.8598557283947572]\n",
      "Current experts' weights for day  38 :  [0.9262142919037677, 0.552757807929424, 0.5465356286896068, 0.383592657432443, 0.8583201358342025]\n",
      "Current experts' weights for day  39 :  [0.9244390899223013, 0.540557406330652, 0.5371843429674061, 0.3721019332824548, 0.8582643890644072]\n",
      "Current experts' weights for day  40 :  [0.9234058278610432, 0.5286635015171355, 0.5270073644768443, 0.3608602880268847, 0.8575015730690349]\n",
      "Current experts' weights for day  41 :  [0.9221039247016, 0.5157287126016719, 0.5160285100365541, 0.3511578206584619, 0.8560570175977366]\n",
      "Current experts' weights for day  42 :  [0.9207424914633695, 0.5035809365840748, 0.5073773792216582, 0.3414024233771042, 0.8549076056446174]\n",
      "Current experts' weights for day  43 :  [0.9205694137240635, 0.49186760792719586, 0.4985733329041758, 0.33200005646158365, 0.8537336720034022]\n",
      "Current experts' weights for day  44 :  [0.920268250268078, 0.4802186581333604, 0.4899917011794791, 0.3230695690773656, 0.8523521578437308]\n",
      "Current experts' weights for day  45 :  [0.9200780095134699, 0.46864132700120154, 0.4814254392922014, 0.3144985231915066, 0.8508489519003134]\n",
      "Current experts' weights for day  46 :  [0.9180708092103551, 0.4576287216271665, 0.47423239699490916, 0.3054655190465852, 0.8504966230527711]\n",
      "Current experts' weights for day  47 :  [0.9164603819062566, 0.4460655691836213, 0.46500738432056377, 0.2977599862591917, 0.8490035339310953]\n",
      "Current experts' weights for day  48 :  [0.9149447110191308, 0.43572759552547, 0.4581766941530833, 0.28965022790258615, 0.8486099805349745]\n",
      "Current experts' weights for day  49 :  [0.914837096259726, 0.425362665282695, 0.4504613593758213, 0.2820528124020573, 0.8478902489921032]\n",
      "Current experts' weights for day  50 :  [0.9144660049499134, 0.41460687710681465, 0.4431006376884328, 0.27476458833417755, 0.8469514821819607]\n",
      "Current experts' weights for day  51 :  [0.9138392844781903, 0.4052134790554243, 0.4369119291588336, 0.2676921692101333, 0.8460362398492576]\n",
      "Current experts' weights for day  52 :  [0.9132771802203882, 0.39509865159989516, 0.429663247950306, 0.2610572350971986, 0.8445785166181004]\n",
      "Current experts' weights for day  53 :  [0.9120055780316056, 0.3854072518104349, 0.42341787734566483, 0.2542669354559206, 0.8443290726120554]\n",
      "Current experts' weights for day  54 :  [0.9112238253686743, 0.37594304803633843, 0.41691216012128585, 0.2474577996993932, 0.8440041962550973]\n",
      "Current experts' weights for day  55 :  [0.910533587404375, 0.36650257552969123, 0.41014856540470984, 0.2407643061868108, 0.8433360634438227]\n",
      "Current experts' weights for day  56 :  [0.9103479297153731, 0.3572132174251899, 0.40291800457144916, 0.2345237718451045, 0.843175129869647]\n",
      "Current experts' weights for day  57 :  [0.910019425200568, 0.34800936004174243, 0.3963367358437612, 0.22866268221097033, 0.8422348025358803]\n",
      "Current experts' weights for day  58 :  [0.9081077985789725, 0.33829629044091886, 0.38947479468040114, 0.22368550758918632, 0.8391236663476145]\n",
      "Current experts' weights for day  59 :  [0.906563087922622, 0.3300110017616728, 0.38401346940818837, 0.21890752096754837, 0.8357283308368256]\n",
      "Current experts' weights for day  60 :  [0.9061218124124651, 0.32322602135812223, 0.37973103661200647, 0.21370051253120068, 0.8345215659328169]\n",
      "Current experts' weights for day  61 :  [0.9058361547442062, 0.31521338551220635, 0.3745216620211733, 0.20871789961200807, 0.8337274268138978]\n",
      "Current experts' weights for day  62 :  [0.9050873141643829, 0.30742547438851425, 0.36911369426382884, 0.20411821817285733, 0.8324987045498635]\n",
      "Current experts' weights for day  63 :  [0.9038860669771803, 0.3005525417888001, 0.36386994313276644, 0.19976720011036375, 0.8311241764878499]\n",
      "Current experts' weights for day  64 :  [0.9012120238335684, 0.2953082374085716, 0.35868257377165746, 0.19600309382890466, 0.8284665222477883]\n",
      "Current experts' weights for day  65 :  [0.8992253583159071, 0.29128927364237767, 0.3542060717640955, 0.1924748227979186, 0.8253095429041377]\n",
      "Current experts' weights for day  66 :  [0.8980287560107912, 0.2880975480527242, 0.3497535074175389, 0.18949766918304045, 0.8207931108060058]\n",
      "Current experts' weights for day  67 :  [0.8966376249331984, 0.28624486126587834, 0.3467039156880037, 0.1862332303686243, 0.8182438671400928]\n",
      "Current experts' weights for day  68 :  [0.8954935089800594, 0.2843762205220746, 0.3436698501658943, 0.18276939135666975, 0.8176982639058612]\n",
      "Current experts' weights for day  69 :  [0.8928570880770172, 0.2823890618220108, 0.3400732173897034, 0.17973584634834314, 0.8164213421372394]\n",
      "Current experts' weights for day  70 :  [0.8924355730861632, 0.28084680651522914, 0.33710013283008106, 0.17641042180463493, 0.8160202404982966]\n",
      "Current experts' weights for day  71 :  [0.8919641375855938, 0.2790494989455732, 0.334447825904939, 0.17265699301230303, 0.8133792725397032]\n",
      "Current experts' weights for day  72 :  [0.8914896816429582, 0.2758732472986544, 0.33145755059543597, 0.16852531877689036, 0.8089820958825544]\n",
      "Current experts' weights for day  73 :  [0.8900547292846325, 0.27294458363215085, 0.3283563349480282, 0.16412687240009224, 0.8039600260999656]\n",
      "Current experts' weights for day  74 :  [0.8860950811954214, 0.2688718736850758, 0.32494386267920605, 0.15907578379010395, 0.7967711163444052]\n",
      "Current experts' weights for day  75 :  [0.8857227034279719, 0.26265229014479863, 0.3198546646637348, 0.15416217588385966, 0.7906154814429647]\n",
      "Current experts' weights for day  76 :  [0.8846467657181641, 0.2569148254266577, 0.3147406225869874, 0.14928784340152781, 0.7859628113550313]\n",
      "Current experts' weights for day  77 :  [0.8841310330045065, 0.250902198705527, 0.30882299698146715, 0.1447382642155797, 0.783693396877213]\n",
      "Current experts' weights for day  78 :  [0.8814444859121509, 0.24561614895327036, 0.30384820085839553, 0.13982600172251647, 0.7806458897140627]\n",
      "Current experts' weights for day  79 :  [0.8792245874968889, 0.24039372866190334, 0.2986028840961125, 0.13491213256670026, 0.7777321603026562]\n",
      "Current experts' weights for day  80 :  [0.8783146737290861, 0.2346093371589796, 0.2922642698074901, 0.1306421078433822, 0.7776026483052257]\n",
      "Current experts' weights for day  81 :  [0.8763730031790395, 0.22959076246144125, 0.2871854575618276, 0.12613214391218955, 0.7765549952702809]\n",
      "Current experts' weights for day  82 :  [0.8746147536462252, 0.22456689414625997, 0.2818331319829279, 0.12170372342486468, 0.7752567751267244]\n",
      "Current experts' weights for day  83 :  [0.8745276609009172, 0.21925904321112522, 0.27597364047456924, 0.11770321722150204, 0.7749099919342876]\n",
      "Current experts' weights for day  84 :  [0.873935706088705, 0.2140519641411673, 0.2705377847916081, 0.11406597694291006, 0.7728886296221406]\n",
      "Current experts' weights for day  85 :  [0.8721736130038973, 0.20958522152873862, 0.26634256801301787, 0.11032948776446344, 0.7720626697134513]\n",
      "Current experts' weights for day  86 :  [0.8719511683427132, 0.20475183850715797, 0.2613494678047857, 0.10687690139862972, 0.770880419940358]\n",
      "Current experts' weights for day  87 :  [0.8705242549504214, 0.200654426051016, 0.2569878964910412, 0.10341224048155057, 0.7702212670715265]\n",
      "Current experts' weights for day  88 :  [0.8662659642359201, 0.1973253812208897, 0.25343132177541405, 0.09949638047284638, 0.7673500664563365]\n",
      "Current experts' weights for day  89 :  [0.8657234853613016, 0.19314307527159846, 0.24822112318315012, 0.09590072125042003, 0.765174001172577]\n",
      "Current experts' weights for day  90 :  [0.8645375410507732, 0.18893017832607417, 0.2433467086348931, 0.09249221217257401, 0.7639140315000702]\n",
      "Current experts' weights for day  91 :  [0.8630747772082898, 0.18473991639975554, 0.23864673700556902, 0.0891967757749455, 0.7632133480112596]\n",
      "Current experts' weights for day  92 :  [0.8624823249562387, 0.180251630390751, 0.2335020979431998, 0.08628990808975692, 0.7618748365605412]\n",
      "Current experts' weights for day  93 :  [0.8619764125991143, 0.17644965550931271, 0.22903282686044138, 0.08360081404420647, 0.7592490121974007]\n",
      "Current experts' weights for day  94 :  [0.8606351173774198, 0.17258247535371815, 0.22536815673742938, 0.08088804543879966, 0.757332304712053]\n",
      "Current experts' weights for day  95 :  [0.8575233687843966, 0.16850024812469716, 0.22066846014389482, 0.07868155763182146, 0.7532515070439431]\n",
      "Current experts' weights for day  96 :  [0.8548625374741615, 0.16452413285963244, 0.2167095508521846, 0.07676485187124514, 0.7479411621897858]\n",
      "Current experts' weights for day  97 :  [0.8519551552254999, 0.1617496003395506, 0.2135978504552409, 0.07504254646176599, 0.7424682091594001]\n",
      "Current experts' weights for day  98 :  [0.8484300178200597, 0.15930900125338282, 0.21069050881975843, 0.07347239348777374, 0.7373821781579706]\n",
      "Current experts' weights for day  99 :  [0.8469575164415074, 0.15719214256164832, 0.2082946723386928, 0.07196886063048358, 0.7331787280631477]\n",
      "Current experts' weights for day  100 :  [0.8445204226584172, 0.15526430214070117, 0.20565209154655714, 0.07064994973015735, 0.7288899281344411]\n",
      "Current experts' weights for day  101 :  [0.843615143202896, 0.15366174175557318, 0.20348073132929087, 0.06938457664721212, 0.7256724183946319]\n",
      "Current experts' weights for day  102 :  [0.843590616241123, 0.15214070832317456, 0.20181284177665118, 0.06806340439452534, 0.724158642002362]\n",
      "Current experts' weights for day  103 :  [0.8434297196604362, 0.15076565427683727, 0.20018834907112648, 0.06670940390037002, 0.7239172535733422]\n",
      "Current experts' weights for day  104 :  [0.8428328926155859, 0.1491313401662304, 0.19841561699430532, 0.06533871786132253, 0.7233598595946354]\n",
      "Current experts' weights for day  105 :  [0.8423262949095325, 0.1473564471738014, 0.19654521044153844, 0.0639591802926993, 0.7223364771154939]\n",
      "Current experts' weights for day  106 :  [0.8416209281261221, 0.14558730458506297, 0.1946630545746201, 0.06257827707311427, 0.7210366350761342]\n",
      "Current experts' weights for day  107 :  [0.8413661694264207, 0.14402169962857356, 0.19301368505415598, 0.061094741914886695, 0.7188075634414852]\n",
      "Current experts' weights for day  108 :  [0.8388051681365665, 0.14253883739276652, 0.19091233129150775, 0.05966830697792287, 0.7172541497481457]\n",
      "Current experts' weights for day  109 :  [0.8381072178194032, 0.14117614749026608, 0.18920242897778697, 0.058110839886368934, 0.7146396691114602]\n",
      "Current experts' weights for day  110 :  [0.8354833027889463, 0.1393344617372293, 0.1866253710940683, 0.05676182745332298, 0.7141410164658873]\n",
      "Current experts' weights for day  111 :  [0.8331739484202243, 0.1374948596763611, 0.1843654739736879, 0.0554259412485031, 0.7140989379628264]\n",
      "Current experts' weights for day  112 :  [0.8309241641048023, 0.135828490177243, 0.1823176329607757, 0.05409825573356976, 0.7140623729464598]\n",
      "Current experts' weights for day  113 :  [0.8273169497648162, 0.13405696992625518, 0.18004770409360857, 0.0528957772235652, 0.7128789682861542]\n",
      "Current experts' weights for day  114 :  [0.8272116127220432, 0.13274929999017174, 0.17837420874434726, 0.051617786854148945, 0.7127778532259557]\n",
      "Current experts' weights for day  115 :  [0.8271737023952226, 0.13178227641958998, 0.17710293790703696, 0.050188724240621894, 0.7107227897999586]\n",
      "Current experts' weights for day  116 :  [0.8258414948440296, 0.13015348372419788, 0.17461179000287375, 0.04887719460930869, 0.7093419734430744]\n",
      "Current experts' weights for day  117 :  [0.8230029601583377, 0.12817620579086297, 0.1720237485691359, 0.04770385537164364, 0.7090020696278485]\n",
      "Current experts' weights for day  118 :  [0.8212199085692973, 0.12666416525889743, 0.1700922987303578, 0.046519858321785894, 0.708831191721875]\n",
      "Current experts' weights for day  119 :  [0.8206329915023763, 0.12541902787806203, 0.1684867483829666, 0.04525759072367558, 0.7076966076956395]\n",
      "Current experts' weights for day  120 :  [0.8185232571783656, 0.12371961006048195, 0.1661910106213639, 0.0441114763126871, 0.7075059426215075]\n",
      "Current experts' weights for day  121 :  [0.8159652339689749, 0.12199714195139155, 0.16403931921193876, 0.04305908019761322, 0.7061604188927171]\n",
      "Current experts' weights for day  122 :  [0.8137903120502676, 0.12029129656402424, 0.1619155222912261, 0.04212882458279827, 0.7038097129390056]\n",
      "Current experts' weights for day  123 :  [0.8134922557010162, 0.11902109688532918, 0.16051009552425147, 0.04117076272344891, 0.7023541196669512]\n",
      "Current experts' weights for day  124 :  [0.8108243220270854, 0.11741121955900313, 0.1586434650265715, 0.040328303337934863, 0.6998942101661422]\n",
      "Current experts' weights for day  125 :  [0.8108115822226333, 0.11586391126717029, 0.15718007817749358, 0.039481485315581454, 0.6979726689968037]\n",
      "Current experts' weights for day  126 :  [0.8104356635891189, 0.11446963124626794, 0.15587192655897716, 0.03859505724798744, 0.6976146644747121]\n",
      "Current experts' weights for day  127 :  [0.8104320521641609, 0.11315927348820215, 0.15457563289661091, 0.037679518891423756, 0.6967616626800971]\n",
      "Current experts' weights for day  128 :  [0.8083746700407779, 0.11173869664669979, 0.15281028302668526, 0.036851667840633644, 0.6967509705364373]\n",
      "Current experts' weights for day  129 :  [0.8075822414282211, 0.1103273465433038, 0.15135641992417564, 0.036028802667188144, 0.6967268606209419]\n",
      "Current experts' weights for day  130 :  [0.8070956203132962, 0.10894596908069538, 0.14998586898525187, 0.035196814902765536, 0.6962284285389798]\n",
      "Current experts' weights for day  131 :  [0.8053930291549545, 0.10748614503364379, 0.1483061902929712, 0.03442365063977333, 0.6958711837747503]\n",
      "Current experts' weights for day  132 :  [0.7983912914527953, 0.10546109131120163, 0.1458579416234483, 0.03397958987484018, 0.6901474415493382]\n",
      "Current experts' weights for day  133 :  [0.7964542937499235, 0.10355012031636654, 0.14490579176554869, 0.03355914119328173, 0.6842259886944227]\n",
      "Current experts' weights for day  134 :  [0.7955148444000333, 0.10202383388192657, 0.14426910694817852, 0.03311340150827951, 0.68006039537162]\n",
      "Current experts' weights for day  135 :  [0.7953181237619605, 0.10050017191154788, 0.14356517476663147, 0.03268847541471885, 0.6767393950480252]\n",
      "Current experts' weights for day  136 :  [0.7950890959007335, 0.09902089881652869, 0.14290115910470225, 0.03226212403901653, 0.6745510362474101]\n",
      "Current experts' weights for day  137 :  [0.7949834286405678, 0.09764686752129707, 0.1422993356666235, 0.031837761016034354, 0.6735578920119302]\n",
      "Current experts' weights for day  138 :  [0.7931856650689273, 0.09609052744028324, 0.14148553555898324, 0.03148555829990709, 0.6723045409799381]\n",
      "Current experts' weights for day  139 :  [0.7924171957173912, 0.09455954320565055, 0.1411381942201134, 0.031131419429238187, 0.6711339946190533]\n",
      "Current experts' weights for day  140 :  [0.791538427675007, 0.09316870676312783, 0.14073785908749184, 0.03078211001883437, 0.6701063220523057]\n",
      "Current experts' weights for day  141 :  [0.7899905146797006, 0.09167550842780836, 0.14022298424770552, 0.03046114037432732, 0.6688480689883616]\n",
      "Current experts' weights for day  142 :  [0.7884509121303406, 0.0903052867310987, 0.1398896605816928, 0.030154128228630193, 0.6675739622002628]\n",
      "Current experts' weights for day  143 :  [0.7867560117789839, 0.08888985204332005, 0.1395610147892494, 0.02985269586553933, 0.666560303445835]\n",
      "Current experts' weights for day  144 :  [0.7852115634412354, 0.08749726602884549, 0.13936717355695277, 0.029543763820703527, 0.6661205092557633]\n",
      "Current experts' weights for day  145 :  [0.784150885957087, 0.08634684436326218, 0.13922822101097596, 0.029218531197672405, 0.6660353602053015]\n",
      "Current experts' weights for day  146 :  [0.7831010738571934, 0.08498187833892444, 0.13891752219635176, 0.028890370548805774, 0.6657081627651308]\n",
      "Current experts' weights for day  147 :  [0.7817226459067244, 0.08366458822682649, 0.1385411049705725, 0.028572219883544723, 0.6654876378587481]\n",
      "Current experts' weights for day  148 :  [0.7801612907466742, 0.0823430879066724, 0.13823612802882335, 0.028271403779985934, 0.6653208372952718]\n",
      "Current experts' weights for day  149 :  [0.7783214861029161, 0.0810830947375495, 0.13795302270797669, 0.027977173501987234, 0.6649696500452336]\n",
      "Current experts' weights for day  150 :  [0.7772739723002237, 0.07979972900314275, 0.13782824775809896, 0.027660067579307834, 0.6648295378439141]\n",
      "Total losses incurred by WMA:  144.2555289112013\n",
      "Total loss incurred by the best expert from hindsight:  56.0\n",
      "Regret =  88.25552891120131\n",
      "Upper bound of mistakes with best expert:  2383.303173079208\n"
     ]
    }
   ],
   "source": [
    "# Input Parameters\n",
    "n = 5  # number of experts\n",
    "T = 150  # time horizon\n",
    "eps =  0.1 # parameter epsilon  \n",
    "\n",
    "from random import random\n",
    "from random import uniform\n",
    "\n",
    "# Initialization\n",
    "w_exp = [1,1,1,1,1]  # numpy array of the experts' weights\n",
    "l_exp = [0,0,0,0,0]  # numpy array of the total losses of experts\n",
    "loss_exp = [0,0,0,0,0]  # numpy array of loss of each experts\n",
    "l_mwu = 0  # the total loss of MWU\n",
    "\n",
    "# Run the multiplicative weight update algorithm\n",
    "for t in range(1, T+1):\n",
    "    # get experts' predictions for day t from the intel_data dictionary in (2)\n",
    "    predRF = intel_data['RF'][t-1]\n",
    "    predKNN = intel_data['KNN'][t-1]\n",
    "    predSVM = intel_data['SVM'][t-1]\n",
    "    predARIMA = intel_data['ARIMA'][t-1]\n",
    "    predMA = intel_data['MA'][t-1]\n",
    "    exp_preds = [predRF, predKNN, predSVM, predARIMA, predMA]\n",
    "    #print(\"exp_preds\", exp_preds)\n",
    "    \n",
    "    # predict the closing price for day t according to MWU\n",
    "    choice = random()*sum(w_exp)  # https://jeremykun.com/tag/multiplicative-weights-update-algorithm/\n",
    "    outcome = 0\n",
    "    for weight in w_exp:\n",
    "        choice -= weight\n",
    "        if choice <= 0:\n",
    "            break\n",
    "        else: outcome = outcome + 1\n",
    "    #print('Predicted Outcome: ', exp_preds[outcome])\n",
    "    \n",
    "    # get the true closing price for day t from the intel_data dictionary in (2)\n",
    "    actual = intel_data['actual'][t-1]\n",
    "    #print(\"Actual Outcome: \", actual)\n",
    "    \n",
    "    # update the experts' weights (w_exp) according to MWU\n",
    "    for i in range(n):\n",
    "        if (exp_preds[i] != actual):\n",
    "            w_exp[i] *= 1-eps*loss(exp_preds[i], actual)\n",
    "    \n",
    "    # update the total losses incurred by experts (l_exp) and MWU (l_mwu)\n",
    "    l = loss(outcome, actual)\n",
    "    l_mwu += l \n",
    "    l_exp[outcome] += 1\n",
    "    loss_exp[outcome] += l\n",
    "    #print(\"Total losses incurred by experts: \", l_exp)\n",
    "    \n",
    "    # print the experts' weights (w_exp) after round t\n",
    "    print(\"Current experts' weights for day \", t, \": \", w_exp)\n",
    "\n",
    "    \n",
    "# print the total loss incurred by MWU over T time periods\n",
    "print(\"Total losses incurred by WMA: \", l_mwu)\n",
    "\n",
    "# print the total loss incurred by the best expert from hindsight\n",
    "bestindex = np.argmax(w_exp)\n",
    "nummax = loss_exp[bestindex]\n",
    "print(\"Total loss incurred by the best expert from hindsight: \", nummax)\n",
    "\n",
    "# print the regret of MWU based on this single instance of stock prices\n",
    "regret = l_mwu - nummax\n",
    "print(\"Regret = \", regret)\n",
    "\n",
    "upperbest = l_mwu*np.log(n)/eps + nummax + nummax*eps\n",
    "print(\"Upper bound of mistakes with best expert: \", upperbest) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Implement the multiplicative weight update algorithm (MWU) on Microsoft stock prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current experts' weights for day  1 :  [0.9974792431726044, 0.9526826114627891, 0.9510142463313793, 0.9993742199519356, 0.9977488665522161]\n",
      "Current experts' weights for day  2 :  [0.993780965445378, 0.9060547815014, 0.9045075848219023, 0.9974005212279897, 0.9946802140809688]\n",
      "Current experts' weights for day  3 :  [0.9902096915368973, 0.8618174757855503, 0.8624855216085043, 0.9958654234746621, 0.9923997924831478]\n",
      "Current experts' weights for day  4 :  [0.9834230214795241, 0.8190268875122915, 0.8212734733192445, 0.9910486175922714, 0.9876909887216409]\n",
      "Current experts' weights for day  5 :  [0.9761609929875708, 0.7794469982791362, 0.7869842564461946, 0.9856956766251473, 0.9831133479294625]\n",
      "Current experts' weights for day  6 :  [0.9692011241165702, 0.7419071673329103, 0.7545292555407994, 0.980608209415692, 0.9797618054622022]\n",
      "Current experts' weights for day  7 :  [0.9618725262571296, 0.7053713620294564, 0.7233096706954252, 0.9749313464389607, 0.9767985091158766]\n",
      "Current experts' weights for day  8 :  [0.9563347768002733, 0.6702793041797387, 0.69478249891746, 0.9712117150598057, 0.9767771240414775]\n",
      "Current experts' weights for day  9 :  [0.9521144422998687, 0.6370229142819336, 0.6666400013177449, 0.9688928012860845, 0.9747076335067053]\n",
      "Current experts' weights for day  10 :  [0.9460134635183993, 0.6049320833725389, 0.6369534780915693, 0.9646601256880285, 0.9742856841404839]\n",
      "Current experts' weights for day  11 :  [0.9405175100134722, 0.5746288336725258, 0.610397015268997, 0.9609636515681796, 0.9737382845732385]\n",
      "Current experts' weights for day  12 :  [0.9369096478184917, 0.5464261505575153, 0.5848595847899796, 0.9593391472230107, 0.9712930757877494]\n",
      "Current experts' weights for day  13 :  [0.9331591234619425, 0.51913314107727, 0.5587335469175011, 0.9578287667052405, 0.9690968158722142]\n",
      "Current experts' weights for day  14 :  [0.9295456884171045, 0.49377834816600985, 0.5335575190964025, 0.9563268291500597, 0.9676821691119671]\n",
      "Current experts' weights for day  15 :  [0.9248005789662439, 0.4691962425170247, 0.5087357003190739, 0.9537070612191154, 0.9674751295755295]\n",
      "Current experts' weights for day  16 :  [0.9226531358317649, 0.44672921713411895, 0.48685542031016954, 0.953350579972159, 0.9648350641045385]\n",
      "Current experts' weights for day  17 :  [0.919513565503532, 0.42606713999945733, 0.46373605338726415, 0.9526755369242522, 0.963572652525471]\n",
      "Current experts' weights for day  18 :  [0.9156354417940434, 0.4052962825824933, 0.4420177299172794, 0.9510941428191402, 0.9631001194521446]\n",
      "Current experts' weights for day  19 :  [0.9097942139569409, 0.385100627695471, 0.42134560800889015, 0.9475259838910617, 0.9603467024884131]\n",
      "Current experts' weights for day  20 :  [0.9044064374247651, 0.36592828694599583, 0.403431615019564, 0.94463789799346, 0.9583040758331713]\n",
      "Current experts' weights for day  21 :  [0.9004084137876873, 0.3485364011047156, 0.38685640720465747, 0.9431480288383122, 0.9581613374216177]\n",
      "Current experts' weights for day  22 :  [0.896012801978298, 0.33182780154056213, 0.36974957465444425, 0.9412746675391083, 0.957647900070345]\n",
      "Current experts' weights for day  23 :  [0.8911836955337633, 0.315461234614223, 0.35361903440110887, 0.938924839548007, 0.9569908173831216]\n",
      "Current experts' weights for day  24 :  [0.8873762538855069, 0.30023819272153096, 0.3386307167942608, 0.9377062505352805, 0.9561959795509631]\n",
      "Current experts' weights for day  25 :  [0.8833991338306956, 0.2856258405733474, 0.3235314985006804, 0.9365412325052623, 0.9551976680612249]\n",
      "Current experts' weights for day  26 :  [0.8796383539198462, 0.27170339169188695, 0.30940046045745484, 0.9355269696662464, 0.9545556220051375]\n",
      "Current experts' weights for day  27 :  [0.8765701537057908, 0.2587967192233526, 0.2960265186420104, 0.9354676901737308, 0.9532552766052468]\n",
      "Current experts' weights for day  28 :  [0.8729810901785292, 0.24635603374317103, 0.2823734865952282, 0.9349024729155542, 0.9525729197691614]\n",
      "Current experts' weights for day  29 :  [0.8709406230135817, 0.23471547252127675, 0.26988458822672123, 0.9338291658719838, 0.9506027694539944]\n",
      "Current experts' weights for day  30 :  [0.8704483344932586, 0.22403830850645337, 0.2576728063535064, 0.9296680437754086, 0.9458354730851982]\n",
      "Current experts' weights for day  31 :  [0.8684567145613312, 0.21382961238989828, 0.244633780488728, 0.9280713129199042, 0.9441536143976292]\n",
      "Current experts' weights for day  32 :  [0.864768195502692, 0.20332845941006397, 0.23242471952870877, 0.9277567264734786, 0.9428330141599424]\n",
      "Current experts' weights for day  33 :  [0.8599545315388228, 0.19358915044116026, 0.22158505024670036, 0.9260707900673103, 0.9396243436320416]\n",
      "Current experts' weights for day  34 :  [0.8529551338637337, 0.18415131148986943, 0.21148646664327286, 0.9219726575520725, 0.9340809938093124]\n",
      "Current experts' weights for day  35 :  [0.8451211208980076, 0.17554230674209498, 0.2027128513081767, 0.9171796802818442, 0.9280693406635182]\n",
      "Current experts' weights for day  36 :  [0.8377484518070611, 0.16767678840947778, 0.19499596858434576, 0.9128133910676118, 0.9235097827087837]\n",
      "Current experts' weights for day  37 :  [0.8308360528682017, 0.16011226800187134, 0.1876988564782724, 0.9089314126640065, 0.9211723852266009]\n",
      "Current experts' weights for day  38 :  [0.8236325679144956, 0.1525961144031592, 0.18037525203710497, 0.904597658416899, 0.9195224582225026]\n",
      "Current experts' weights for day  39 :  [0.816417713234747, 0.1453309495781476, 0.17357011078274429, 0.9002760596918544, 0.9185727398609902]\n",
      "Current experts' weights for day  40 :  [0.8090813779109999, 0.13831163404545835, 0.1670842083264664, 0.8957778373732794, 0.9179725717715932]\n",
      "Current experts' weights for day  41 :  [0.800552436342485, 0.13157155134094217, 0.16075616914848365, 0.8899174114624061, 0.915956781353724]\n",
      "Current experts' weights for day  42 :  [0.791673326034136, 0.1275436331244707, 0.15502892980423016, 0.8838450859023347, 0.9136292576449377]\n",
      "Current experts' weights for day  43 :  [0.7822825040609396, 0.12359485663839617, 0.14949439898093744, 0.8771177413817584, 0.9108967134254334]\n",
      "Current experts' weights for day  44 :  [0.7728309672014468, 0.11951533284719094, 0.1444957329033488, 0.8704937469664599, 0.9087180643727778]\n",
      "Current experts' weights for day  45 :  [0.7630613699043511, 0.11575543132365088, 0.1396379149047519, 0.8635180163380844, 0.9066339105731844]\n",
      "Current experts' weights for day  46 :  [0.755795453579989, 0.1123863528350289, 0.1354120238606523, 0.8592525592744849, 0.9052110673834637]\n",
      "Current experts' weights for day  47 :  [0.7481391976063473, 0.1087276918346692, 0.13072800781395044, 0.854765624745292, 0.903467282155719]\n",
      "Current experts' weights for day  48 :  [0.7405725179810175, 0.10537346512588866, 0.1261381062126389, 0.8502394653376543, 0.9021108996265348]\n",
      "Current experts' weights for day  49 :  [0.7332912640396998, 0.1023120022409693, 0.12169078779710892, 0.845917605251199, 0.9008637363562649]\n",
      "Current experts' weights for day  50 :  [0.7250129881778883, 0.0991426529157317, 0.11726339969795824, 0.8406603159694687, 0.9005795625675993]\n",
      "Current experts' weights for day  51 :  [0.7165859987897614, 0.09592294852554875, 0.11321394163358939, 0.8349819284727767, 0.8993069583407101]\n",
      "Current experts' weights for day  52 :  [0.7079594882609598, 0.09299522578730744, 0.10933839782073515, 0.8289082319051321, 0.8971937827115127]\n",
      "Current experts' weights for day  53 :  [0.6996185709689782, 0.09020862604497867, 0.10575847335565018, 0.8231624480012302, 0.8957536641282599]\n",
      "Current experts' weights for day  54 :  [0.6914209829018908, 0.08734580591332228, 0.10224773076754057, 0.8175748696115942, 0.8948419480967191]\n",
      "Current experts' weights for day  55 :  [0.68354310515272, 0.08479219207269098, 0.09887759307966154, 0.8123688268521531, 0.8946373491779157]\n",
      "Current experts' weights for day  56 :  [0.6751442718502492, 0.08217246073982135, 0.09551491634333974, 0.8064880282944383, 0.8939221555390225]\n",
      "Current experts' weights for day  57 :  [0.6665122819514979, 0.07961110275382591, 0.0924236859765766, 0.8004118838256364, 0.892955899408491]\n",
      "Current experts' weights for day  58 :  [0.657781405665227, 0.07718694775126198, 0.08949767259734649, 0.794176775191254, 0.8918360150158]\n",
      "Current experts' weights for day  59 :  [0.6492629708671621, 0.07499970667536889, 0.08671357589724646, 0.788048705783458, 0.8908382427078243]\n",
      "Current experts' weights for day  60 :  [0.6416052358850765, 0.07282926717259496, 0.0841236204705765, 0.7829914976835569, 0.8904262270371932]\n",
      "Current experts' weights for day  61 :  [0.6342767061653815, 0.07086463527127902, 0.08148866269666956, 0.7784631489143208, 0.889229150389422]\n",
      "Current experts' weights for day  62 :  [0.6270663985014387, 0.06891042947511644, 0.07883228989752074, 0.7739395832591897, 0.888051941864904]\n",
      "Current experts' weights for day  63 :  [0.6196610025190185, 0.06681315576045062, 0.07616839674396847, 0.7693927443503121, 0.8872134194261199]\n",
      "Current experts' weights for day  64 :  [0.6116285263343864, 0.06480750643564466, 0.07370756635984173, 0.763983867619649, 0.8866263481682012]\n",
      "Current experts' weights for day  65 :  [0.6034137918810374, 0.06294130733469602, 0.07132708300056809, 0.7582319049169701, 0.8852056107545638]\n",
      "Current experts' weights for day  66 :  [0.5948673556657434, 0.06105221829519204, 0.06907770898443809, 0.7518653926142378, 0.8828428801179085]\n",
      "Current experts' weights for day  67 :  [0.5866350080040632, 0.059289982620926406, 0.06699404495542653, 0.745868222195428, 0.8810896599805695]\n",
      "Current experts' weights for day  68 :  [0.5781053342067216, 0.05748598830420235, 0.06494872045954506, 0.739415818662604, 0.8791920817996696]\n",
      "Current experts' weights for day  69 :  [0.5697818592593482, 0.05574017194576928, 0.06306734608033893, 0.7332213012288907, 0.8779577328891333]\n",
      "Current experts' weights for day  70 :  [0.5618407011070057, 0.05424013174103114, 0.06125517978866561, 0.727457837352634, 0.8777350318979212]\n",
      "Current experts' weights for day  71 :  [0.5554579545584101, 0.05272660622542804, 0.05958432560568505, 0.7238024508710215, 0.8750014131445883]\n",
      "Current experts' weights for day  72 :  [0.5483378257976677, 0.0514080592162649, 0.057623907641163465, 0.7193281530815492, 0.8733117447024322]\n",
      "Current experts' weights for day  73 :  [0.5417632758462863, 0.05003404856352325, 0.05584632249026896, 0.715507158006343, 0.8714388083566839]\n",
      "Current experts' weights for day  74 :  [0.536958965224252, 0.048568903349205855, 0.0541888017074154, 0.7141662640553418, 0.8666930602962063]\n",
      "Current experts' weights for day  75 :  [0.5312962524718726, 0.04645688258104988, 0.05216144901781155, 0.7115924818374988, 0.8642272750131921]\n",
      "Current experts' weights for day  76 :  [0.5254859800776115, 0.044635421218732575, 0.05032049410825598, 0.7087826843659316, 0.8632830627558384]\n",
      "Current experts' weights for day  77 :  [0.5185227846011593, 0.04247869074064577, 0.04845398371229696, 0.7042674488746273, 0.8612542959543721]\n",
      "Current experts' weights for day  78 :  [0.5120954919864601, 0.04125871233615073, 0.0468998347338259, 0.7003991373946603, 0.8598271024083374]\n",
      "Current experts' weights for day  79 :  [0.5064065921748963, 0.040215318901272896, 0.04544585068107278, 0.6975409537704531, 0.8597029990649364]\n",
      "Current experts' weights for day  80 :  [0.4999439123092386, 0.03880633178501258, 0.04388408431813057, 0.693389963074724, 0.8579389312622335]\n",
      "Current experts' weights for day  81 :  [0.4949903719417964, 0.03780952651854575, 0.04254812224640057, 0.6915662124966819, 0.8563431801708085]\n",
      "Current experts' weights for day  82 :  [0.4901048015704495, 0.03637045667695645, 0.041114142854001855, 0.6899516376386813, 0.854053699795531]\n",
      "Current experts' weights for day  83 :  [0.48452809401283503, 0.0348098907298696, 0.039616265735684025, 0.6871803344456472, 0.8535116456780971]\n",
      "Current experts' weights for day  84 :  [0.4783301521354939, 0.033357990537980296, 0.03823112183334434, 0.6833852383453842, 0.8519959056072891]\n",
      "Current experts' weights for day  85 :  [0.47256098899528887, 0.03243254092229486, 0.03702734473830865, 0.6802034487371982, 0.8509878636912734]\n",
      "Current experts' weights for day  86 :  [0.46647380869491895, 0.03151920016720524, 0.03581628943816102, 0.6763720154269942, 0.8493752896177214]\n",
      "Current experts' weights for day  87 :  [0.4606994972691811, 0.030648636089089232, 0.03471608835164174, 0.6730809672771922, 0.8482113899964502]\n",
      "Current experts' weights for day  88 :  [0.45634744626468315, 0.02986731307330658, 0.03369875229005017, 0.671965535360462, 0.8460086735793548]\n",
      "Current experts' weights for day  89 :  [0.45126438671704716, 0.028804041169560018, 0.0325341050295536, 0.6698980252565814, 0.8446195815210807]\n",
      "Current experts' weights for day  90 :  [0.44629401377027267, 0.027648611598963858, 0.031420240045589284, 0.6677492483091766, 0.8437571704524712]\n",
      "Current experts' weights for day  91 :  [0.4413629409010873, 0.02672600546058571, 0.030370761373212686, 0.665744213729103, 0.8431509309165569]\n",
      "Current experts' weights for day  92 :  [0.4358248659267555, 0.025673277219860575, 0.029289666323589557, 0.6625980136106613, 0.8419971861426094]\n",
      "Current experts' weights for day  93 :  [0.4304281609342361, 0.024959096068163256, 0.02834961669699454, 0.6596354175152112, 0.8406070898416116]\n",
      "Current experts' weights for day  94 :  [0.4255372992070365, 0.024150182208213192, 0.027478299612633972, 0.6575793414741781, 0.8403673436698094]\n",
      "Current experts' weights for day  95 :  [0.4203682997383286, 0.02321482973488424, 0.026551280107357203, 0.6548429818823293, 0.8397054508836624]\n",
      "Current experts' weights for day  96 :  [0.41462394441370093, 0.02229112641567374, 0.02566450171908037, 0.6510818401668015, 0.8376752856152353]\n",
      "Current experts' weights for day  97 :  [0.4091215502585642, 0.021717342205716406, 0.02490004135327962, 0.6478099030365164, 0.8364359311725559]\n",
      "Current experts' weights for day  98 :  [0.4041807029359296, 0.021016803105983965, 0.024167020318712415, 0.6453099670552435, 0.8361804779779157]\n",
      "Current experts' weights for day  99 :  [0.3995948533326367, 0.020214927412819078, 0.02343432726124856, 0.6435251435179739, 0.8349171132637894]\n",
      "Current experts' weights for day  100 :  [0.395050626646067, 0.019437487376269724, 0.022659481760597056, 0.6417758036519228, 0.833722267092923]\n",
      "Current experts' weights for day  101 :  [0.39022021835889825, 0.01867600195762642, 0.021891032995671453, 0.6394438874432595, 0.8333851891646067]\n",
      "Current experts' weights for day  102 :  [0.38548037464641316, 0.017939066916459405, 0.02119641715774621, 0.6372582606174417, 0.8331096628831707]\n",
      "Current experts' weights for day  103 :  [0.3810623218746757, 0.017240347277247735, 0.020529087861240754, 0.635575397506817, 0.8325364530108446]\n",
      "Current experts' weights for day  104 :  [0.37649204138449116, 0.01656894125347442, 0.019838812685577276, 0.6334674260340051, 0.8322468034025675]\n",
      "Current experts' weights for day  105 :  [0.37170627949101537, 0.01590538218429162, 0.0191863291640569, 0.6309140465032103, 0.8311377005876595]\n",
      "Current experts' weights for day  106 :  [0.3663402906753167, 0.015266212859333845, 0.018555313668741767, 0.6273227061529102, 0.8287020815771631]\n",
      "Current experts' weights for day  107 :  [0.36136057812529593, 0.01490683714864468, 0.018030596882058082, 0.624489931763668, 0.8274992043578486]\n",
      "Current experts' weights for day  108 :  [0.35651587834486453, 0.01444390454629063, 0.017497276091450624, 0.6218448869102324, 0.8268961569040228]\n",
      "Current experts' weights for day  109 :  [0.3522103754144132, 0.013896922317354007, 0.016982208280360754, 0.62002072659208, 0.8261919301976647]\n",
      "Current experts' weights for day  110 :  [0.3474091330285745, 0.01345416731785031, 0.01644800057026371, 0.6173928712230824, 0.8260449047916741]\n",
      "Current experts' weights for day  111 :  [0.342730315795259, 0.012915913889430098, 0.015934064855270266, 0.6147456227506308, 0.8257903607641469]\n",
      "Current experts' weights for day  112 :  [0.33851472101831215, 0.012533678650640399, 0.01546897924055521, 0.6129290734850663, 0.8248622193781286]\n",
      "Current experts' weights for day  113 :  [0.33402295836650564, 0.012052678562207628, 0.014987364494226102, 0.6106073001659658, 0.8248278086867629]\n",
      "Current experts' weights for day  114 :  [0.3300555133849917, 0.011584183083024868, 0.014532897785630113, 0.609180424974413, 0.8238890733309214]\n",
      "Current experts' weights for day  115 :  [0.32663730158114057, 0.011250035586469233, 0.01410346697637477, 0.608872898240696, 0.8214672708413414]\n",
      "Current experts' weights for day  116 :  [0.3228517164241293, 0.01080719131798552, 0.013616783300484698, 0.6078871404724506, 0.8201013734142215]\n",
      "Current experts' weights for day  117 :  [0.3187810090179773, 0.010383847113260868, 0.013145802029906069, 0.6061517892071917, 0.819794994882715]\n",
      "Current experts' weights for day  118 :  [0.3149647978886416, 0.00998801599036503, 0.012731855160260376, 0.6048965105174567, 0.8196945556433756]\n",
      "Current experts' weights for day  119 :  [0.31153867570047833, 0.00960879973675554, 0.012334986242778397, 0.6043230235853024, 0.818872866789639]\n",
      "Current experts' weights for day  120 :  [0.307586974396408, 0.00922949400388035, 0.011916248669309753, 0.6026824454102897, 0.8179298320031174]\n",
      "Current experts' weights for day  121 :  [0.3035651453216325, 0.008866484902673876, 0.011533770412821762, 0.6007286005226818, 0.8163233674306106]\n",
      "Current experts' weights for day  122 :  [0.29948633665975416, 0.008512917336668355, 0.01117700087249936, 0.5985841876841497, 0.8148073747952381]\n",
      "Current experts' weights for day  123 :  [0.29544203666567326, 0.008185058678150094, 0.01084713671890171, 0.5965455492182794, 0.8136979970910366]\n",
      "Current experts' weights for day  124 :  [0.29113094302774184, 0.007857644757928698, 0.01051350950202711, 0.593735213835992, 0.8116242325841172]\n",
      "Current experts' weights for day  125 :  [0.28707481611153507, 0.007549992839022819, 0.01021456156379264, 0.5914492277824372, 0.81049011882168]\n",
      "Current experts' weights for day  126 :  [0.2832157967879639, 0.007304586986744417, 0.009920599092758881, 0.5895089303719028, 0.8104541464984962]\n",
      "Current experts' weights for day  127 :  [0.27977272806015985, 0.007075365093422749, 0.009643125394296119, 0.5885654125400588, 0.8088899785345115]\n",
      "Current experts' weights for day  128 :  [0.2761461196950743, 0.0067994907779912175, 0.0093422788162923, 0.5872406165681494, 0.8078673933634877]\n",
      "Current experts' weights for day  129 :  [0.27299103599367125, 0.006542235006665739, 0.009062990000357643, 0.5868304343867562, 0.8058792652250842]\n",
      "Current experts' weights for day  130 :  [0.2696646534497797, 0.006337611378945428, 0.00877747827997754, 0.5860953323588378, 0.804551010770196]\n",
      "Current experts' weights for day  131 :  [0.26589212277573254, 0.006088051503957273, 0.008481837007491007, 0.5842877938575444, 0.8037012692325042]\n",
      "Current experts' weights for day  132 :  [0.2620439900110686, 0.0059277496943736345, 0.008221963664554582, 0.5822108694930728, 0.8020439551590725]\n",
      "Current experts' weights for day  133 :  [0.2577418407294283, 0.005684970530037237, 0.007964445328627109, 0.5788356511198928, 0.7985329605413868]\n",
      "Current experts' weights for day  134 :  [0.2536759235851595, 0.0055448264220367294, 0.007760303278470394, 0.576041093343658, 0.7961356264230083]\n",
      "Current experts' weights for day  135 :  [0.2494859481046227, 0.005418140114546198, 0.007545790686364878, 0.5726263864166937, 0.7934291721608088]\n",
      "Current experts' weights for day  136 :  [0.24554531832176185, 0.005295828558143865, 0.007352078276898815, 0.5697391179241139, 0.7921098374782033]\n",
      "Current experts' weights for day  137 :  [0.24159220666963077, 0.005116635581323357, 0.007159672732802268, 0.5667756312209067, 0.7914153908422987]\n",
      "Current experts' weights for day  138 :  [0.2375702494080609, 0.004928412055387646, 0.006969582381582894, 0.5635595735199255, 0.7906466947557818]\n",
      "Current experts' weights for day  139 :  [0.23369770127567568, 0.004803222998876147, 0.006794369323743649, 0.5604352101628896, 0.7902548157703109]\n",
      "Current experts' weights for day  140 :  [0.22994259106313594, 0.004686776694314309, 0.006624138658518984, 0.5575153715822608, 0.7901209685669836]\n",
      "Current experts' weights for day  141 :  [0.22622106272197057, 0.0045730835465174905, 0.006453375394007386, 0.5545774261646186, 0.7900563427380997]\n",
      "Current experts' weights for day  142 :  [0.22233892722098755, 0.004446083021054229, 0.006283171477574971, 0.551082355414324, 0.7890127462362522]\n",
      "Current experts' weights for day  143 :  [0.2185045043014905, 0.004323253556250729, 0.006127928140338271, 0.5475952938218033, 0.7879867217578514]\n",
      "Current experts' weights for day  144 :  [0.2146181771775164, 0.004207712992215491, 0.005975624994921415, 0.5438339564265716, 0.7866943930556345]\n",
      "Current experts' weights for day  145 :  [0.21077222654373318, 0.004093172710448514, 0.005834018587358941, 0.5400541032331337, 0.7854447412515184]\n",
      "Current experts' weights for day  146 :  [0.20690756085582326, 0.0039817065981193195, 0.0056952739759297995, 0.5360836768427814, 0.7840824426596106]\n",
      "Current experts' weights for day  147 :  [0.2028985587932394, 0.0038695858536621007, 0.005559839210630029, 0.531546834996385, 0.7820810643172503]\n",
      "Current experts' weights for day  148 :  [0.19889902571037693, 0.003754803255119953, 0.005439304395523094, 0.5269651893814926, 0.780292591574295]\n",
      "Current experts' weights for day  149 :  [0.19497302177452744, 0.0036571444108265756, 0.005324777463029994, 0.5224432466794033, 0.778898972082095]\n",
      "Current experts' weights for day  150 :  [0.1912050468776837, 0.0035644936654454793, 0.005218394136607775, 0.5182420001754369, 0.778341948934465]\n",
      "Total losses incurred by WMA:  146.97725790876234\n",
      "Total loss incurred by the best expert from hindsight:  61.16466704739838\n",
      "Regret =  85.81259086136396\n",
      "Upper bound of mistakes with best expert:  2432.788845191806\n"
     ]
    }
   ],
   "source": [
    "# Input Parameters\n",
    "n = 5  # number of experts\n",
    "T = 150  # time horizon\n",
    "eps =  0.1 # parameter epsilon \n",
    "\n",
    "from random import random\n",
    "from random import uniform\n",
    "\n",
    "# Initialization\n",
    "w_exp = [1,1,1,1,1]  # numpy array of the experts' weights\n",
    "l_exp = [0,0,0,0,0]  # numpy array of the total losses of experts\n",
    "loss_exp = [0,0,0,0,0]  # numpy array of loss of each experts\n",
    "l_mwu = 0  # the total loss of MWU\n",
    "\n",
    "# Run the multiplicative weight update algorithm\n",
    "for t in range(1, T+1):\n",
    "    # get experts' predictions for day t from the intel_data dictionary in (2)\n",
    "    predRF = microsoft_data['RF'][t-1]\n",
    "    predKNN = microsoft_data['KNN'][t-1]\n",
    "    predSVM = microsoft_data['SVM'][t-1]\n",
    "    predARIMA = microsoft_data['ARIMA'][t-1]\n",
    "    predMA = microsoft_data['MA'][t-1]\n",
    "    exp_preds = [predRF, predKNN, predSVM, predARIMA, predMA]\n",
    "    #print(\"exp_preds\", exp_preds)\n",
    "    \n",
    "    # predict the closing price for day t according to MWU\n",
    "    choice = random()*sum(w_exp)  # https://jeremykun.com/tag/multiplicative-weights-update-algorithm/\n",
    "    outcome = 0\n",
    "    for weight in w_exp:\n",
    "        choice -= weight\n",
    "        if choice <= 0:\n",
    "            break\n",
    "        else: outcome += 1\n",
    "    #print('Predicted Outcome: ', exp_preds[outcome])\n",
    "    \n",
    "    # get the true closing price for day t from the intel_data dictionary in (2)\n",
    "    actual = microsoft_data['actual'][t-1]\n",
    "    #print(\"Actual Outcome: \", actual)\n",
    "    \n",
    "    # update the experts' weights (w_exp) according to MWU\n",
    "    for i in range(n):\n",
    "        if (exp_preds[i] != actual):\n",
    "            w_exp[i] *= 1-eps*loss(exp_preds[i], actual)\n",
    "    \n",
    "    # update the total losses incurred by experts (l_exp) and MWU (l_mwu)\n",
    "    l = loss(outcome, actual)\n",
    "    l_mwu += l \n",
    "    l_exp[outcome] += 1\n",
    "    loss_exp[outcome] += l\n",
    "    #print(\"Total losses incurred by experts: \", l_exp)\n",
    "    \n",
    "    # print the experts' weights (w_exp) after round t\n",
    "    print(\"Current experts' weights for day \", t, \": \", w_exp)\n",
    "\n",
    "    \n",
    "# print the total loss incurred by MWU over T time periods\n",
    "print(\"Total losses incurred by WMA: \", l_mwu)\n",
    "\n",
    "# print the total loss incurred by the best expert from hindsight\n",
    "bestindex = np.argmax(w_exp)\n",
    "nummax = loss_exp[bestindex]\n",
    "print(\"Total loss incurred by the best expert from hindsight: \", nummax)\n",
    "\n",
    "# print the regret of MWU based on this single instance of stock prices\n",
    "regret = l_mwu - nummax\n",
    "print(\"Regret = \", regret)\n",
    "\n",
    "upperbest = l_mwu*np.log(n)/eps + nummax + nummax*eps\n",
    "print(\"Upper bound of mistakes with best expert: \", upperbest) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
